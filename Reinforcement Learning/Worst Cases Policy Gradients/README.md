# Worst Cases Policy Gradients

ğŸ“” Peng Zhenghao

ğŸ–‡ï¸ [https://arxiv.org/pdf/1911.03618.pdf](https://arxiv.org/pdf/1911.03618.pdf)

ğŸ–‹ï¸ Tang, Yichuan Charlie, Jian Zhang, and Ruslan Salakhutdinov. "Worst cases policy gradients." arXiv preprint arXiv:1911.03618 (2019).

ğŸ« Apple

![Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled.png](Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled.png)


## Highlights


- åˆ©ç”¨åˆ†å¸ƒå¼RLçš„è§†è§’ï¼Œæå‡ºä¸€ç§èƒ½å¤ŸåŠ¨æ€æ§åˆ¶ç­–ç•¥é£é™©åŒæ¶ç¨‹åº¦çš„æ–¹æ³•
- åŸºäºDDPGç®—æ³•ï¼Œå…¶Criticä¸ä»…è¾“å‡ºQå€¼ï¼Œè€Œä¸”è¾“å‡ºQå€¼çš„æ–¹å·®ï¼Œåˆ©ç”¨Wassersteinè·ç¦»æ¥æ›´æ–°Criticè¾“å‡ºçš„æ–¹å·®é¡¹
- Returnåˆ†å¸ƒçš„ç™¾åˆ†æ¯”alphaä½œä¸ºè¡¡é‡riskçš„æŒ‡æ ‡ï¼Œåœ¨è®­ç»ƒæ—¶éšæœºé‡‡æ ·ï¼Œåœ¨æµ‹è¯•æ—¶å¯ä»¥æ‰‹åŠ¨æ¡ä»¶ç­–ç•¥çš„é£é™©åŒæ¶ç¨‹åº¦ã€‚

## Formulation

- åœ¨RLä¸­å¦‚ä½•å¼•å…¥é£é™©è¿™ä¸ªæ¦‚å¿µï¼Ÿå¦‚ä½•æœ€å°åŒ–é£é™©ï¼Ÿ
- æˆ‘ä»¬èƒ½å¦å¾—åˆ°ä¸€ç§åŠæ³•ä»¥åŠ¨æ€çš„è°ƒæ•´ç­–ç•¥å¯¹é£é™©çš„åå¥½ï¼Ÿ
- è®¾å›æŠ¥Returnæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œç»™å®šä¸€ä¸ª0ã€1ä¹‹é—´çš„æ•°alphaï¼Œç°å¸Œæœ›æœ€å¤§åŒ–Returnçš„alpha-ç™¾åˆ†æ•°ã€‚å½“alphaè¶‹äº0çš„æ—¶å€™ï¼Œåˆ™æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–åœ¨â€éå¸¸å°‘è§çš„ã€å›æŠ¥å¾ˆä½çš„â€œæƒ…å†µä¸‹çš„å›æŠ¥ã€‚æ­¤æ—¶å³æ˜¯Worst Casesã€‚

## Method


### è®­ç»ƒæ¨¡å‹æ¥æ‹Ÿåˆå›æŠ¥çš„åˆ†å¸ƒ

- å‡è®¾return(s,a)çš„åˆ†å¸ƒæ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå…¶å‡å€¼ä¸ºä¼ ç»Ÿçš„Q(s,a)å‡½æ•°ç»™å‡ºçš„å€¼ï¼Œè€Œå…¶æ–¹å·®è¡¨ç¤ºä¸º $\Upsilon(s,a) = E[R^2] - Q^2(s,a)$
- ä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥æ‹Ÿåˆæ–¹å·®ï¼š

![Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%201.png](Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%201.png)

- ä½¿ç”¨TD learningæ¥å­¦ä¹ æ–¹å·®ï¼š

    ç”¨Wasserstein Metricåšlossã€‚

    ![Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%202.png](Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%202.png)

    F-1æ˜¯inverse cumulative distribution functionã€‚è®¾ä¸€ä¸ªè€çš„é«˜æ–¯åˆ†å¸ƒuå’Œä¸€ä¸ªæ–°çš„é«˜æ–¯åˆ†å¸ƒvåˆ†åˆ«æœä»ï¼šu~N(Î¼1, C1), v~N(Î¼2, C2), åˆ™äºŒè€…çš„Wassersteinè·ç¦»ä¸º

    ![Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%203.png](Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%203.png)

### CVaR

- å› ä¸ºæˆ‘ä»¬å·²ç»å‡è®¾äº†å›æŠ¥çš„åˆ†å¸ƒæ˜¯ä¸€ä¸ªé«˜æ–¯å‡½æ•°ï¼Œå¯ä»¥ç”±Qå’ŒÎ¥åˆ†åˆ«è¡¨ç¤ºå‡å€¼å’Œæ–¹å·®ï¼Œæ‰€ä»¥è¿™ä¸ªåˆ†å¸ƒçš„alpha-percentileçš„å¯ä»¥ç›´æ¥å†™å‡ºæ¥

![Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%204.png](Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%204.png)

- ç°åœ¨åŸå§‹çš„policy gradientå°±å¯ä»¥å†™æˆï¼ˆå…¶å®å°±æ˜¯ç”¨ä¸Šå¼æ›¿æ¢äº†Qï¼‰ä¸‹å¼ã€‚ä»è€Œå¯ä»¥æŠŠ $\Gamma$ å±•å¼€æ¥å†™ä»è€Œå¾—åˆ°æœ€ç»ˆçš„Jã€‚

![Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%205.png](Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%205.png)

- ä¸€ä¸ªæ–°é—®é¢˜äº§ç”Ÿäº†ï¼Œæ–°çš„Jä¸alphaæœ‰å…³ï¼Œè¯¥é€‰å“ªä¸ªalphaå‘¢ï¼Ÿç›´è§‚çš„åšæ³•æ˜¯å°†alphaç¦»æ•£åŒ–åˆ°0~1ç›´æ¥çš„Nä¸ªåŒºé—´ï¼Œå¹¶è®­Nä¸ªä¸åŒçš„ç­–ç•¥ï¼Œä½†è¿™æ ·å‚æ•°å¤ªå¤šäº†ã€‚æœ¬æ–‡çš„åšæ³•æ˜¯å°†alphaä½œä¸ºè¾“å…¥ä¼ å…¥ç­–ç•¥ç½‘ç»œä¸­ã€‚åœ¨æ¯ä¸ªepisodeå¼€å§‹å‰ï¼Œå‡åŒ€éšæœºé‡‡æ ·ä¸€ä¸ªalphaï¼Œå¹¶åœ¨è¿™ä¸ªepisodeä¸­å›ºå®šä½å®ƒã€‚

## Experiment


### Experimental Setting

- ä¸¤ä¸ªè¿ç»­åŠ¨ä½œçš„é©¾é©¶ç¯å¢ƒï¼š1. è½¬å¼¯å’Œ2. æ±‡å…¥

![Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%206.png](Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%206.png)

- ç¯å¢ƒçš„å…·ä½“è®¾è®¡
    1. æœ¬è½¦
        1. è¿åŠ¨å­¦æ¨¡å‹ä¸ºç¦»æ•£æ—¶é—´çš„kinematics bicycle model
        2. è½¬å‘ç”±Stanley controlleræ§åˆ¶ï¼ˆä¸€ç§éçº¿æ€§é—­ç¯è½¬å‘æ§åˆ¶å™¨ï¼‰
        3. åˆå§‹é€Ÿåº¦ä¸º5~20m/séšæœºé‡‡æ ·
    2. ç¯å¢ƒ
        1. å¤§æ¦‚200x200ç±³
        2. ç¢°æ’reward -50
        3. æˆåŠŸreward $50\times e^{-t/50} + 10$ï¼Œtè¡¨ç¤ºç›®å‰çš„step
        4. æ²¡æœ‰æ“ä½œæˆåŠŸåˆ™æ˜¯ 0
    3. ä»–è½¦
        1. rule-basedè¡Œä¸º
        2. é€Ÿåº¦éšæœºé€‰å–
        3. å¯ä»¥æ‰§è¡Œadaptive curise controlï¼šåŸºäºå‰è½¦çš„æƒ…å†µè‡ªåŠ¨åŠ å‡é€Ÿ
        4. å¯ä»¥å®‰å…¨çš„å˜é“ï¼šåŠ¨æ€è§„åˆ’å‡ºä¸€æ¡å¹³æ»‘çš„å˜é“è½¨è¿¹
        5. æ—¢ç„¶å°è½¦æœ‰è¿™äº›èƒ½åŠ›äº†ï¼Œä»–ä»¬åœ¨å‡ºç”Ÿçš„æ—¶å€™éšæœºé€‰å–ä¸‰ç§æ€§æ ¼ï¼šyieldï¼ˆç¤¼è®©çš„ï¼‰ï¼Œignoreï¼ˆåˆ†å¿ƒçš„ï¼‰ï¼Œaccelerateï¼ˆè·¯æ€’ç—‡ï¼‰
- ç½‘ç»œçš„å…·ä½“è®¾è®¡ï¼š
    1. alphaä½œä¸ºactorå’Œcriticçš„è¾“å…¥
    2. criticä¸ä»…è¾“å‡ºå‡å€¼Qï¼Œè€Œä¸”è¾“å‡ºæ–¹å·®ã€‚æ–¹å·®ä½¿ç”¨softpluså‡½æ•°ä¿è¯æ’å¤§äº0ã€‚
    3. è¾“å‡ºçš„æ˜¯è·ç¦»æœ¬è½¦æœ€è¿‘çš„ä¸€äº›è½¦çš„stateå¦‚é€Ÿåº¦æ–¹å‘ä½ç½®ç­‰ã€‚è¾“å‡ºä¸ºå°è½¦çš„åŠ é€Ÿåº¦ã€‚

![Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%207.png](Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%207.png)

### Result

- ä¸‹å›¾å·¦ï¼šé¢„æµ‹çš„Rçš„æ–¹å·®éšç€alphaçš„å¢å¤§è€Œå¢å¤§ã€‚ä¸­ï¼šéšç€alphaçš„å¢åŠ episode lengthå˜å°ï¼Œè¯´æ˜è½¦å¼€çš„è¶Šæ¥è¶Šå¿«ã€‚å³å›¾ï¼šè¾“å‡ºçš„æ–¹å·®éšç€alphaçš„å¢å¤§è€Œå¢å¤§ã€‚

![Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%208.png](Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%208.png)

- ä¸‹å›¾å±•ç¤ºæ•°å€¼ç»“æœã€‚æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ç§settingã€‚æ‹¬å·å·¦è¾¹çš„ç™¾åˆ†æ¯”è¡¨ç¤ºç¢°æ’ç‡ï¼Œæ‹¬å·å†…çš„è¡¨ç¤ºæˆåŠŸç‡ã€‚å› ä¸ºalphaåœ¨è®­ç»ƒçš„æ—¶å€™ä½œä¸ºè¾“å…¥ä¼ è¿›ç½‘ç»œï¼Œæ‰€ä»¥å¯ä»¥åœ¨æµ‹è¯•çš„æ—¶å€™è°ƒæ•´alphaçš„å€¼ä»è€Œç”»å‡ºä¸åŒçš„åˆ—ï¼Œè¿™ç‚¹å¾ˆå¥½ã€‚

![Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%209.png](Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%209.png)

- ä½œè€…ä¹Ÿåœ¨Carlaä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œæ‰‹å·¥æå–Carlaä¸­è½¦çš„ä¿¡æ¯å¹¶ä¼ ç»™ç­–ç•¥ç½‘ç»œã€‚
- ç¬”è€…æ³¨ï¼šä½œè€…æ²¡æœ‰ä¸å…¶ä»–Risk-averseçš„ç®—æ³•è¿›è¡Œå¯¹æ¯”ã€‚

## Related Works

### Risk-sensitive, Safe RL, Robust MDP

- [ ]  L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta. Robust adversarial reinforcement learning. *ICML*,
2017. URL https://arxiv.org/abs/1703.02702.

åˆ†æˆä¸¤ç±»

- ä¿®æ”¹æ¢ç´¢è¿‡ç¨‹
    - åˆ©ç”¨å¤–éƒ¨çŸ¥è¯†
    - risk-directed exploration
- ä¿®æ”¹è®­ç»ƒæ—¶çš„æœ€ä¼˜æ¡ä»¶ã€‚æœ¬ç¯‡å·¥ä½œå°±æ˜¯è¿™ç±»ã€‚

### Distributional RL

- [ ]  M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning.
*CoRR*, abs/1707.06887, 2017. URL http://arxiv.org/abs/1707.06887.
- [ ]  W. Dabney, M. Rowland, M. G. Bellemare, and R. Munos. Distributional reinforcement learning with
quantile regression. *arXiv preprint arXiv:1710.10044*, 2017.
- [ ]  G. Barth-Maron, M. W. Hoffman, D. Budden, W. Dabney, D. Horgan, A. Muldal, N. Heess, and T. Lilli- crap. Distributed distributional deterministic policy gradients. arXiv preprint arXiv:1804.08617, 2018.

## Remained Questions


### Distributional RL

å®šä¹‰ï¼Œå†…æ¶µï¼Œç”¨å¤„ï¼Œæœªæ¥ã€‚

### Wasserstein Metric

- ä¸ºä»€ä¹ˆç”¨å®ƒä¸ç”¨KLï¼Ÿ
- ä»–çš„è¡¨è¾¾å¼æ˜¯æ€ä¹ˆè®¡ç®—çš„ï¼Ÿ

![Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%2010.png](Worst%20Cases%20Policy%20Gradients%20a6790e03d8194ee197326a0fb81c67ff/Untitled%2010.png)

- æˆ‘ä¼°è®¡è¿™ä¸ªä¸œè¥¿è‚¯å®šæ˜¯distributional RLçš„é‡ç‚¹ï¼Œå¯ä»¥äº†è§£ä¸€ä¸‹